{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "prompt_tuner is set up for convenient text generation with transformers generation and sampling tools.\n",
    "\n",
    "You can load a SoftPrompt object and simply concatenate it into your context string, where it shows up as a human-readable tag that tokenizes to the underlying number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "%pip install transformers\n",
    "%pip install git+https://github.com/AlpinDale/prompt-tuner.git#egg=master --log PIP_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from prompt_tuner.inference import LlamaSoftPromptLM\n",
    "from prompt_tuner.tokenizers import LlamaSPTokenizerFast\n",
    "from prompt_tuner.soft_prompt import SoftPrompt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to instantiate prompt_tuner's model and tokenizer classes.\n",
    "model = GPT2SoftPromptLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "tokenizer = LlamaSoftPromptLM.from_pretrained(\"NousResearch/Llama-2-7b-hf\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftPrompts may be loaded in several ways.\n",
    "# sp = SoftPrompt.from_json(json_str)\n",
    "# sp = SoftPrompt.from_inputs_embeds(inputs_embeds)\n",
    "# sp = SoftPrompt.from_tuning_model(model)\n",
    "sp = SoftPrompt.from_string(\"With the court firmly balkanized into three distinct factions, Princess Charlotte had her work cut out for her.\",\n",
    "                             model=model, tokenizer=tokenizer)\n",
    "\n",
    "#sp = SoftPrompt.from_file(\"sample_sps/finetune/neuromancer_NousResearch/Llama-2-7b-hf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about a SoftPrompt can be printed with\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'get_tag_str()' to get a tag string that you can add to your context.\n",
    "prompt = sp.get_tag_str() + \"She\"\n",
    "\n",
    "# The addition operator also works:\n",
    "# prompt = sp + \"Armitage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tag string conveniently contains the SP's name, part of its GUID,\n",
    "# and a series of '@'s which represent individual soft tokens.\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tag string tokenizes to the correct length budget your context.\n",
    "prompt_len = len(tokenizer.encode(prompt))\n",
    "print(f\"Length of soft prompt: {len(tokenizer.encode(sp.get_tag_str()))}\")\n",
    "print(f\"Length of full prompt: {prompt_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation is as usual.\n",
    "output = generator( prompt,\n",
    "                    do_sample=True,\n",
    "                    min_length=prompt_len+100,\n",
    "                    max_length=prompt_len+100,\n",
    "                    repetition_penalty=1.7,\n",
    "                    top_p=0.8,\n",
    "                    temperature=0.7,\n",
    "                    use_cache=True,\n",
    "                    return_full_text=True)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also fine to use generate() instead of a pipeline.\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# This also demonstrates multiple return sequences.\n",
    "output = model.generate(input_ids,\n",
    "                        do_sample=True,\n",
    "                        min_length=prompt_len+100,\n",
    "                        max_length=prompt_len+100,\n",
    "                        repetition_penalty=1.7,\n",
    "                        top_p=0.8,\n",
    "                        temperature=0.7,\n",
    "                        use_cache=True,\n",
    "                        return_full_text=True,\n",
    "                        num_return_sequences=3)\n",
    "\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(tokenizer.decode(output[1]))\n",
    "print(tokenizer.decode(output[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you decide to write your own sampler instead of a pipeline,\n",
    "# be aware that the special token logits will be very high.\n",
    "# This is accounted for when using model.generate() or pipelines,\n",
    "# but you will need to exclude special tokens when sampling.\n",
    "\n",
    "output = model(input_ids)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
