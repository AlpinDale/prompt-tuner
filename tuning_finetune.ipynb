{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja7zPK-Gvi7Z"
      },
      "source": [
        "# Prompt Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE2jNS5UMXKh",
        "outputId": "525b14e8-82f3-471c-a90e-6a54dca12ca4"
      },
      "outputs": [],
      "source": [
        "#@title Colab-specific setup\n",
        "#@markdown This will ask for you to log into Google Drive.\n",
        "#@markdown Click on the link and copy over your access token.\n",
        "\n",
        "import torch\n",
        "colab = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if colab:\n",
        "    !nvidia-smi\n",
        "    gpu_type = torch.cuda.get_device_name(0)\n",
        "    if gpu_type != 'Tesla T4':\n",
        "        raise ValueError(\"Highly advised to use a T4.\")\n",
        "\n",
        "# Setup for Colab only\n",
        "if colab:\n",
        "    !pip install transformers\n",
        "    !pip install git+https://github.com/AlpinDale/prompt-tuner.git#egg=master --log PIP_LOG\n",
        "    !pip install gdown\n",
        "    !pip install datasets\n",
        "    !pip install tqdm\n",
        "\n",
        "    # Add word wrapping to outputs\n",
        "    from IPython.display import HTML, display\n",
        "    def set_css():\n",
        "      display(HTML('''\n",
        "      <style>\n",
        "        pre {\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "      </style>\n",
        "      '''))\n",
        "    get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "# If on Colab, mount your Google Drive first!\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jFjyHZRoimo5",
        "outputId": "604d668e-54fd-4b26-a5a9-ab5d51beebb2"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Grab text from Project Gutenberg\n",
        "#@markdown We'll be using \"Alice's Adventures in Wonderland\" by Lewis Caroll.\n",
        "import requests, re\n",
        "data_str = requests.get(\"https://www.gutenberg.org/files/11/11-0.txt\").content.decode(\"utf-8\")\n",
        "\n",
        "# Do a little cleanup\n",
        "clean_data_str = data_str\n",
        "\n",
        "def regex_replace(str, regex, group, replacement):\n",
        "    pat = re.compile(regex)\n",
        "    while True:\n",
        "        m = pat.search(str)\n",
        "        if m is not None:\n",
        "            str = str[:m.start(group)] + replacement + str[m.end(group):]\n",
        "        else:\n",
        "            break\n",
        "    return str\n",
        "\n",
        "# Remove carriage returns\n",
        "clean_data_str = regex_replace(clean_data_str, r\"\\r\", 0, \"\")\n",
        "\n",
        "# Replace single newlines with spaces\n",
        "clean_data_str = regex_replace(clean_data_str, r\"\\S(\\n)\\S\", 1, \" \")\n",
        "\n",
        "# Remove left quotes\n",
        "clean_data_str = regex_replace(clean_data_str, r\"\\u201C\", 0, '\"')\n",
        "\n",
        "# Remove right quotes\n",
        "clean_data_str = regex_replace(clean_data_str, r\"\\u201D\", 0, '\"')\n",
        "\n",
        "# Remove italics\n",
        "clean_data_str = regex_replace(clean_data_str, r\"_\", 0, '')\n",
        "\n",
        "# Remove header and footer\n",
        "clean_data_str = clean_data_str[1434:-18595]\n",
        "\n",
        "print(clean_data_str)\n",
        "\n",
        "with open(\"alice.txt\", \"w\") as file:\n",
        "    file.write(clean_data_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YjCfqiHmy1fA",
        "outputId": "af6902ce-3a7b-41fc-890a-116073a26574"
      },
      "outputs": [],
      "source": [
        "#@title Load tokenizer\n",
        "from transformers import LlamaTokenizerFast\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tWYdCgONMXKm",
        "outputId": "2f1f6c44-e2c9-4f92-9bdc-d810d2b3bc35"
      },
      "outputs": [],
      "source": [
        "#-----------------------#\n",
        "#  Training Parameters  #\n",
        "#-----------------------#\n",
        "\n",
        "# Use a string to set the initial value of the soft prompt.\n",
        "# Be aware of the number of tokens.\n",
        "initial_prompt = \"A surreal children's fantasy story set in a subterranean world populated by peculiar anthropomorphic creatures.\\n\"\n",
        "\n",
        "print(f\"Initial prompt length: {len(tokenizer.encode(initial_prompt))} tokens\")\n",
        "\n",
        "# Decide the length of your training blocks in tokens.\n",
        "# Safe sizes for gpt-neo-2.7B-halved:\n",
        "#  - 700 on a Colab T4 (16GB)\n",
        "#  - 400 on a Colab K80 (12GB)\n",
        "#  - 32 on a GTX1080 (8GB)\n",
        "# If it seems a bit small, don't worry!\n",
        "# Soft prompts can be moved forward in context for the best effect.\n",
        "block_size = 32\n",
        "\n",
        "# Name your soft prompt project.\n",
        "sp_name = 'alice-cyclic-dropout-2'\n",
        "\n",
        "# What's the name of model you'll be using?\n",
        "# e.g. gpt2, gpt2-large, gpt-neo-2.7B\n",
        "# (This will be added to the project directory and soft prompt name)\n",
        "model_name = 'Llama-2-7b-hf'\n",
        "\n",
        "# Specify the model directory or huggingface name.\n",
        "model_dir = 'NousResearch/Llama-2-7b-hf'\n",
        "\n",
        "# The above model_dir will download GPT2 1.5B from Huggingface as a baseline.\n",
        "# It is recommended to use finetuneanon's FP16 fork of gpt-neo-2.7B, which can be downloaded from this magnet link:\n",
        "# magnet:?xt=urn:btih:f50bb4e259d2f96aa9151443950b0d2b899a097c&dn=gpt-neo-2.7B-halved&tr=http%3A%2F%2Fopenbittorrent.com%3A80%2Fannounce&tr=http%3A%2F%2Ft.nyaatracker.com%3A80%2Fannounce&tr=udp%3A%2F%2Fopen.stealth.si%3A80%2Fannounce\n",
        "# Once you've saved it to your local machine, create a 'models' folder in your Google Drive and upload it there,\n",
        "# then uncomment the following:\n",
        "#model_dir = \"/content/drive/MyDrive/models/gpt-neo-2.7B-halved/\"\n",
        "\n",
        "# Should be 'gpt2' or 'gpt-neo'.\n",
        "model_type = 'llama'\n",
        "\n",
        "# Specify the path to the text file used for training.\n",
        "text_path = \"alice.txt\"\n",
        "# You can also use something uploaded to your Google Drive, e.g.\n",
        "#text_path = \"/content/drive/MyDrive/datasets/nm_burning_chrome.txt\"\n",
        "\n",
        "# Specify the project directory.\n",
        "project_dir = f\"output/{sp_name}-{model_name}/\"\n",
        "\n",
        "# Checkpoint interval in steps.\n",
        "checkpoint_interval = 20\n",
        "\n",
        "# Evaluation interval in steps.\n",
        "eval_interval = 5\n",
        "\n",
        "# How many blocks to use for evaluation.\n",
        "eval_blocks = 16\n",
        "\n",
        "# Adafactor hyperparameters\n",
        "optimizer_params = {\n",
        "    # Fixed learning rate, recommend 1e-4 to 1e-3\n",
        "    \"lr\": 2e-4,\n",
        "    \n",
        "    # 1st momentum, recommend 0\n",
        "    \"beta1\": 0.0,\n",
        "\n",
        "    # 2nd momentum decay schedule, recommend -0.3 (lower is slower)\n",
        "    \"decay_rate\": -0.8,\n",
        "\n",
        "    # Weight decay, recommend 1e-5\n",
        "    \"weight_decay\": 0.1,\n",
        "    \n",
        "    # Update scaling, recommend False\n",
        "    \"scale_parameter\": False,\n",
        "    \n",
        "    # Built-in LR scheduler, recommend False\n",
        "    \"relative_step\": False\n",
        "    }\n",
        "\n",
        "# LR scheduler parameters\n",
        "scheduler_params = {\n",
        "    \"num_warmup_steps\": 10,\n",
        "    \"num_cycles\": 8,\n",
        "    \"num_training_steps\": 400\n",
        "}\n",
        "\n",
        "# (Use these for GPT-Neo)\n",
        "#scheduler_params = {\n",
        "#    \"num_warmup_steps\": 10,\n",
        "#    \"num_cycles\": 4,\n",
        "#    \"num_training_steps\": 240\n",
        "#}\n",
        "\n",
        "base_acc_steps = 16\n",
        "acc_doubling_rate = 0\n",
        "plateau_steps = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "kEVELdEDttvO",
        "outputId": "2c85cad4-e91e-4ca5-bf0e-1ae2e3fc8d8b"
      },
      "outputs": [],
      "source": [
        "#@title Load model\n",
        "\n",
        "from prompt_tuner.tuning import AutoPromptTuningLM\n",
        "\n",
        "\n",
        "model = AutoPromptTuningLM.from_pretrained(model_dir).half().to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "BZAT90ZCMXKo",
        "outputId": "455d3100-04f5-49db-c96d-3e3b95ddf270"
      },
      "outputs": [],
      "source": [
        "#@title Initialize project\n",
        "#@markdown This will load the latest checkpoint if the project directory already exists.\n",
        "\n",
        "from prompt_tuner.soft_prompt import SoftPrompt\n",
        "from transformers import Adafactor\n",
        "import os\n",
        "\n",
        "filename_for_checkpoint = lambda step: f\"{sp_name}-{model_name}-step-{step}.json\"\n",
        "loaded_sp = None\n",
        "project_files = None\n",
        "\n",
        "# Look for existing project directory\n",
        "try:\n",
        "    os.makedirs(project_dir)\n",
        "    print(f\"Created project directory at {project_dir}\")\n",
        "except FileExistsError:\n",
        "    print(f\"Found project directory at {project_dir}\")\n",
        "\n",
        "# Look for existing checkpoints\n",
        "project_files = os.listdir(project_dir)\n",
        "if project_files is not None:\n",
        "    checkpoint_files = [check_file for check_file in project_files if ('-step-' in check_file) ]\n",
        "\n",
        "    if len(checkpoint_files) > 0:\n",
        "        highest_step = max([ int(check_file[check_file.rfind('-step-')+6:-5]) for check_file in checkpoint_files ])\n",
        "        loaded_sp = SoftPrompt.from_file( os.path.join(project_dir, filename_for_checkpoint(highest_step)) )\n",
        "        print(f\"Loading latest checkpoint: {highest_step}\")\n",
        "    else:\n",
        "        print(\"No checkpoints found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "pf7wZnxNtR29",
        "outputId": "0f1eb6bf-edce-43cb-9201-a5e995ea8879",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#@title Process dataset\n",
        "#@markdown This will load an existing set\n",
        "#@markdown of tokens if present in the project directory.\n",
        "\n",
        "import json\n",
        "import math\n",
        "\n",
        "text_tokenized = None\n",
        "tokens_path = os.path.join(project_dir,\"tokens.json\")\n",
        "\n",
        "# See if we already have a tokens file\n",
        "try:\n",
        "    with open(tokens_path, 'r', encoding='utf-8') as file:\n",
        "        text_tokenized = json.load(file)\n",
        "        print(\"Loaded existing tokens.json file\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"No tokens.json exists, creating it...\")\n",
        "\n",
        "# If not, make one now\n",
        "if text_tokenized is None:\n",
        "\n",
        "    with open(text_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    text_tokenized = tokenizer.encode(text)\n",
        "    \n",
        "    with open(tokens_path, 'x', encoding='utf-8') as file:\n",
        "        json.dump(text_tokenized, file)\n",
        "\n",
        "text_length = len(text_tokenized)\n",
        "num_blocks = math.ceil(text_length/block_size)\n",
        "\n",
        "print(f\"Length of text: {len(text_tokenized)} tokens\")\n",
        "print(f\"Number of blocks: {num_blocks}, each {block_size} tokens\")\n",
        "\n",
        "# Partition tokens into blocks\n",
        "blocks = list()\n",
        "for block_num in range(num_blocks):\n",
        "    start = block_num * block_size\n",
        "    end = min(start + block_size, text_length)\n",
        "    blocks.append( text_tokenized[start:end] )\n",
        "\n",
        "block_order_path = os.path.join(project_dir, \"block_order.json\")\n",
        "\n",
        "# See if we already have a block_order file\n",
        "try:\n",
        "    with open(block_order_path, 'r', encoding='utf-8') as file:\n",
        "        block_order = json.load(file)\n",
        "        print(\"Loaded existing block_order.json file\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"No block_order.json exists, creating it...\")\n",
        "    block_order = [*range(num_blocks)]\n",
        "\n",
        "    with open(block_order_path, 'x', encoding='utf-8') as file:\n",
        "        json.dump(block_order, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TI_f3H5lXeLt",
        "outputId": "e03e52db-a742-4020-e671-129a27b00b1d"
      },
      "outputs": [],
      "source": [
        "#@title Initialize soft prompt in model\n",
        "#@markdown If a checkpoint is present, use that.\n",
        "if loaded_sp is None:\n",
        "    initial_sp = SoftPrompt.from_string(initial_prompt, model, tokenizer)\n",
        "    print(f\"Initial prompt length: {len(initial_sp)}\")\n",
        "    model.set_soft_prompt(initial_sp)\n",
        "\n",
        "    sp_step = 0\n",
        "    eval_loss = 100\n",
        "else:\n",
        "    model.set_soft_prompt(loaded_sp)\n",
        "    sp_step = loaded_sp._metadata['step']\n",
        "    eval_loss = loaded_sp._metadata['loss']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "nYaLKE1YtR3C",
        "outputId": "dea1b23d-285b-4f5f-d242-834e63a03e3e"
      },
      "outputs": [],
      "source": [
        "# Configure number of steps to train for.\n",
        "# One step is (acc_steps) forward passes.\n",
        "num_training_steps = scheduler_params['num_training_steps']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LlqN3HcGQMZ0",
        "outputId": "aeffd993-9b58-491a-8311-1e6db863ac21"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, Adafactor\n",
        "import transformers\n",
        "\n",
        "# Feed soft params to optimizer\n",
        "optimizer_params['params'] = [model.get_soft_params()]\n",
        "optimizer = Adafactor(**optimizer_params)\n",
        "optimizer.state['step'] = sp_step\n",
        "\n",
        "scheduler_params['optimizer'] = optimizer\n",
        "scheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(**scheduler_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "563f05f9831a4913a9fb9fb7116374c4",
            "367016fea14b4eabaf0a1a1c424e63a4",
            "57bcb1b7ab324955ac48b7e22c51d695",
            "034feddd372b458d8e82848fc4db02fc",
            "870cc0cf45cb4895b400ac2c0d1df2d4",
            "7668769f21704c5b9903086b69c15029",
            "16ff7090ebcd4388b1a747060b99a327",
            "9396770b5f4842ac8e01c56dc5b80909"
          ]
        },
        "id": "ImdPj_CftR3C",
        "outputId": "89ee86d1-4273-420a-bce4-37201109d3c0",
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [],
      "source": [
        "#@title Train the soft prompt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import torch\n",
        "import math\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "loss_log_path = os.path.join(project_dir,\"loss_log.csv\")\n",
        "bar = tqdm(total=num_training_steps)\n",
        "optimizer.state['step'] = sp_step\n",
        "evals_since_last_improvement = 0\n",
        "best_eval = float('inf')\n",
        "\n",
        "# Fix eval order\n",
        "eval_order = [*range(num_blocks)]\n",
        "random.seed(1234)\n",
        "random.shuffle(eval_order)\n",
        "\n",
        "# Function for gradient accumulation scheduling\n",
        "def get_acc_steps(sp_step):\n",
        "    if acc_doubling_rate != 0:\n",
        "        return round(base_acc_steps * math.pow(2, (sp_step / acc_doubling_rate)))\n",
        "    else:\n",
        "        return base_acc_steps\n",
        "\n",
        "for session_step in range(num_training_steps):\n",
        "      model.train()\n",
        "\n",
        "      acc_steps = get_acc_steps(sp_step)\n",
        "\n",
        "      for i in range(acc_steps):\n",
        "          idx = (sp_step*acc_steps + i) % num_blocks\n",
        "\n",
        "          # Shuffle blocks every epoch\n",
        "          if idx == 0:\n",
        "              random.shuffle(block_order)\n",
        "              with open(block_order_path, 'w', encoding='utf-8') as file:\n",
        "                  json.dump(block_order, file)\n",
        "\n",
        "          block = blocks[block_order[idx]]\n",
        "\n",
        "          input_ids = torch.LongTensor(block).unsqueeze(0).cuda().detach()\n",
        "          \n",
        "          # Forward pass and optimize\n",
        "          outputs = model(input_ids=input_ids, labels=input_ids)\n",
        "          loss = outputs.loss\n",
        "          loss.backward()\n",
        "\n",
        "          instant_loss = loss.item()\n",
        "          if math.isnan(instant_loss):\n",
        "              torch.cuda.empty_cache()\n",
        "              raise KeyboardInterrupt\n",
        "\n",
        "          # Discard tensor that was moved to GPU\n",
        "          del input_ids\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      # Accumulate gradients\n",
        "      optimizer.step()\n",
        "      lr = optimizer.param_groups[0][\"lr\"]\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if math.isnan(instant_loss):\n",
        "          torch.cuda.empty_cache()\n",
        "          raise KeyboardInterrupt\n",
        "\n",
        "      # Evaluate model and plot loss\n",
        "      if sp_step%eval_interval == 0:\n",
        "          model.eval()\n",
        "          torch.cuda.empty_cache()\n",
        "          eval_loss = 0\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for eval_step in range(eval_blocks):\n",
        "                  block = blocks[eval_order[eval_step]]\n",
        "                  input_ids = torch.LongTensor(block).unsqueeze(0).cuda().detach()\n",
        "                  eval_loss += model(input_ids=input_ids, labels=input_ids).loss.item()\n",
        "                  \n",
        "                  # Discard tensor that was moved to GPU\n",
        "                  del input_ids\n",
        "                  torch.cuda.empty_cache()\n",
        "\n",
        "          eval_loss /= eval_blocks\n",
        "\n",
        "          with open(loss_log_path, 'a', encoding='utf-8') as file:\n",
        "              file.write(f\"{sp_step},{eval_loss}\\n\")\n",
        "          \n",
        "          # Stop if loss has plateaued\n",
        "          if plateau_steps != 0:\n",
        "              if eval_loss < best_eval:\n",
        "                  best_eval = eval_loss\n",
        "                  evals_since_last_improvement = 0\n",
        "              else:\n",
        "                  evals_since_last_improvement += 1\n",
        "              if evals_since_last_improvement > plateau_steps:\n",
        "                  print(f\"No improvement for {plateau_steps} evals\")\n",
        "                  break\n",
        "\n",
        "      # Save checkpoint every so often\n",
        "      if sp_step%checkpoint_interval == 0:\n",
        "          sp = SoftPrompt.from_tuning_model(model,\n",
        "              {\"name\" : sp_name + f\"-step-{sp_step}\",\n",
        "               \"step\"  : sp_step,\n",
        "               \"loss\"  : eval_loss})\n",
        "          sp.to_file( os.path.join( project_dir,filename_for_checkpoint(sp_step) ) )\n",
        "\n",
        "      bar.set_postfix({\n",
        "          \"Model Step\" : sp_step,\n",
        "          \"Eval Loss\"  : \"{el:.5f}\".format(el=eval_loss),\n",
        "          \"Acc Steps\"  : acc_steps,\n",
        "          \"LR\"         : lr\n",
        "      })\n",
        "      bar.update(1)\n",
        "      sp_step += 1\n",
        "\n",
        "# Save a checkpoint once done\n",
        "sp = SoftPrompt.from_tuning_model(model,\n",
        "    {\"name\"  : sp_name + f\"-step-{sp_step}\",\n",
        "     \"step\"  : sp_step,\n",
        "     \"loss\"  : eval_loss})\n",
        "sp.to_file( os.path.join( project_dir,filename_for_checkpoint(sp_step) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-UdO6kRiaptn",
        "outputId": "d06311f9-23b6-4f74-83cc-4e899a9f9579"
      },
      "outputs": [],
      "source": [
        "#@title Flush memory after interrupting training\n",
        "#@markdown This will *hopefully* prevent a CUDA out-of-memory error.\n",
        "try:\n",
        "  del input_ids\n",
        "except Exception:\n",
        "  pass\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "JPZpBzbvaoru",
        "outputId": "5c8fc5d2-ee82-4c94-c2a7-2d1e9eb52f5f"
      },
      "outputs": [],
      "source": [
        "# Plot loss\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cbook as cbook\n",
        "import numpy as np\n",
        "\n",
        "fname2 = cbook.get_sample_data(loss_log_path, asfileobj=False)\n",
        "with cbook.get_sample_data(loss_log_path) as file:\n",
        "    array = np.loadtxt(file, delimiter=\",\")\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(array[:, 0], array[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "YKypaYDOtR3E",
        "outputId": "43002925-1b61-4775-9bbb-1415bfc72c46"
      },
      "outputs": [],
      "source": [
        "# Try generating with your model\n",
        "model.eval()\n",
        "\n",
        "# Restore soft prompt from checkpoint\n",
        "# (Use above graph to find a good stopping point and check project directory for valid checkpoints)\n",
        "sp = SoftPrompt.from_file( os.path.join(project_dir, filename_for_checkpoint(400)) )\n",
        "model.set_soft_prompt(sp)\n",
        "\n",
        "test = \"Alice sipped her tea as the white rabbit gloated about his vast collection of pocket watches\"\n",
        "\n",
        "call = tokenizer(test, return_tensors=\"pt\").input_ids.cuda()\n",
        "\n",
        "basic_output = model.generate(\n",
        "    input_ids=call,\n",
        "    do_sample=True,\n",
        "    min_length=call.shape[-1] + 200,\n",
        "    max_length=call.shape[-1] + 200,\n",
        "    temperature=1.0,\n",
        "    tfs = 0.9,\n",
        "    repetition_penalty = 3.0,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(tokenizer.decode(basic_output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "4RnpmIbbjUTI",
        "outputId": "93d9b3d1-636c-413e-e86b-c20a1b3afe8d"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# Purge soft prompt for comparison.\n",
        "model.initialize_soft_prompt(n_tokens=1)\n",
        "\n",
        "test = \"Alice sipped her tea as the white rabbit gloated about his vast collection of pocket watches\"\n",
        "\n",
        "call = tokenizer(test, return_tensors=\"pt\").input_ids.cuda()\n",
        "\n",
        "basic_output = model.generate(\n",
        "    input_ids=call,\n",
        "    do_sample=True,\n",
        "    min_length=call.shape[-1] + 200,\n",
        "    max_length=call.shape[-1] + 200,\n",
        "    temperature=1.0,\n",
        "    tfs = 0.9,\n",
        "    repetition_penalty = 3.0,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(tokenizer.decode(basic_output[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tuning_finetune_alice.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "metadata": {
      "interpreter": {
        "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
      }
    },
    "orig_nbformat": 2,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "034feddd372b458d8e82848fc4db02fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9396770b5f4842ac8e01c56dc5b80909",
            "placeholder": "​",
            "style": "IPY_MODEL_16ff7090ebcd4388b1a747060b99a327",
            "value": " 400/400 [12:21&lt;00:00,  1.77s/it, Model Step=399, Eval Loss=2.95178, Acc Steps=16, LR=2.08e-7]"
          }
        },
        "16ff7090ebcd4388b1a747060b99a327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "367016fea14b4eabaf0a1a1c424e63a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563f05f9831a4913a9fb9fb7116374c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57bcb1b7ab324955ac48b7e22c51d695",
              "IPY_MODEL_034feddd372b458d8e82848fc4db02fc"
            ],
            "layout": "IPY_MODEL_367016fea14b4eabaf0a1a1c424e63a4"
          }
        },
        "57bcb1b7ab324955ac48b7e22c51d695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7668769f21704c5b9903086b69c15029",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_870cc0cf45cb4895b400ac2c0d1df2d4",
            "value": 400
          }
        },
        "7668769f21704c5b9903086b69c15029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "870cc0cf45cb4895b400ac2c0d1df2d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "9396770b5f4842ac8e01c56dc5b80909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
